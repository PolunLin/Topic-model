{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXHLDxJdRzBi"
      },
      "source": [
        "\n",
        "## **BERTopic**\n",
        "BERTopic is a topic modeling technique that leverages ü§ó transformers and a custom class-based TF-IDF to create dense clusters allowing for easily interpretable topics whilst keeping important words in the topic descriptions. \n",
        "\n",
        "<br>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "SNa-KtKDRnus"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: bertopic in c:\\users\\pc127\\anaconda3\\lib\\site-packages (0.9.4)\n",
            "Requirement already satisfied: pyyaml<6.0 in c:\\users\\pc127\\anaconda3\\lib\\site-packages (from bertopic) (5.3.1)\n",
            "Requirement already satisfied: plotly>=4.7.0 in c:\\users\\pc127\\anaconda3\\lib\\site-packages (from bertopic) (4.14.3)\n",
            "Requirement already satisfied: pandas>=1.1.5 in c:\\users\\pc127\\anaconda3\\lib\\site-packages (from bertopic) (1.3.5)\n",
            "Requirement already satisfied: hdbscan>=0.8.27 in c:\\users\\pc127\\anaconda3\\lib\\site-packages (from bertopic) (0.8.28)\n",
            "Requirement already satisfied: tqdm>=4.41.1 in c:\\users\\pc127\\anaconda3\\lib\\site-packages (from bertopic) (4.49.0)\n",
            "Requirement already satisfied: sentence-transformers>=0.4.1 in c:\\users\\pc127\\anaconda3\\lib\\site-packages (from bertopic) (2.2.0)\n",
            "Requirement already satisfied: umap-learn>=0.5.0 in c:\\users\\pc127\\anaconda3\\lib\\site-packages (from bertopic) (0.5.3)\n",
            "Collecting numpy>=1.20.0\n",
            "  Downloading numpy-1.23.1-cp38-cp38-win_amd64.whl (14.7 MB)\n",
            "     ---------------------------------------- 14.7/14.7 MB 4.1 MB/s eta 0:00:00\n",
            "Requirement already satisfied: scikit-learn>=0.22.2.post1 in c:\\users\\pc127\\anaconda3\\lib\\site-packages (from bertopic) (0.23.2)\n",
            "Requirement already satisfied: scipy>=1.0 in c:\\users\\pc127\\anaconda3\\lib\\site-packages (from hdbscan>=0.8.27->bertopic) (1.5.2)\n",
            "Requirement already satisfied: cython>=0.27 in c:\\users\\pc127\\anaconda3\\lib\\site-packages (from hdbscan>=0.8.27->bertopic) (0.29.14)\n",
            "Requirement already satisfied: joblib>=1.0 in c:\\users\\pc127\\anaconda3\\lib\\site-packages (from hdbscan>=0.8.27->bertopic) (1.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\pc127\\anaconda3\\lib\\site-packages (from pandas>=1.1.5->bertopic) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in c:\\users\\pc127\\anaconda3\\lib\\site-packages (from pandas>=1.1.5->bertopic) (2021.3)\n",
            "Requirement already satisfied: six in c:\\users\\pc127\\anaconda3\\lib\\site-packages (from plotly>=4.7.0->bertopic) (1.15.0)\n",
            "Requirement already satisfied: retrying>=1.3.3 in c:\\users\\pc127\\anaconda3\\lib\\site-packages (from plotly>=4.7.0->bertopic) (1.3.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\pc127\\anaconda3\\lib\\site-packages (from scikit-learn>=0.22.2.post1->bertopic) (2.1.0)\n",
            "Requirement already satisfied: sentencepiece in c:\\users\\pc127\\anaconda3\\lib\\site-packages (from sentence-transformers>=0.4.1->bertopic) (0.1.95)\n",
            "Requirement already satisfied: torchvision in c:\\users\\pc127\\anaconda3\\lib\\site-packages (from sentence-transformers>=0.4.1->bertopic) (0.8.2)\n",
            "Requirement already satisfied: huggingface-hub in c:\\users\\pc127\\anaconda3\\lib\\site-packages (from sentence-transformers>=0.4.1->bertopic) (0.4.0)\n",
            "Requirement already satisfied: nltk in c:\\users\\pc127\\anaconda3\\lib\\site-packages (from sentence-transformers>=0.4.1->bertopic) (3.5)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in c:\\users\\pc127\\anaconda3\\lib\\site-packages (from sentence-transformers>=0.4.1->bertopic) (4.18.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in c:\\users\\pc127\\anaconda3\\lib\\site-packages (from sentence-transformers>=0.4.1->bertopic) (1.7.1)\n",
            "Requirement already satisfied: numba>=0.49 in c:\\users\\pc127\\anaconda3\\lib\\site-packages (from umap-learn>=0.5.0->bertopic) (0.51.2)\n",
            "Requirement already satisfied: pynndescent>=0.5 in c:\\users\\pc127\\anaconda3\\lib\\site-packages (from umap-learn>=0.5.0->bertopic) (0.5.6)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in c:\\users\\pc127\\anaconda3\\lib\\site-packages (from numba>=0.49->umap-learn>=0.5.0->bertopic) (0.34.0)\n",
            "Requirement already satisfied: setuptools in c:\\users\\pc127\\anaconda3\\lib\\site-packages (from numba>=0.49->umap-learn>=0.5.0->bertopic) (50.3.1.post20201107)\n",
            "Requirement already satisfied: typing-extensions in c:\\users\\pc127\\anaconda3\\lib\\site-packages (from torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (3.7.4.3)\n",
            "Requirement already satisfied: requests in c:\\users\\pc127\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (2.26.0)\n",
            "Requirement already satisfied: sacremoses in c:\\users\\pc127\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (0.0.43)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in c:\\users\\pc127\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (0.11.6)\n",
            "Requirement already satisfied: filelock in c:\\users\\pc127\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (3.6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\pc127\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (21.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\pc127\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (2020.10.15)\n",
            "Requirement already satisfied: click in c:\\users\\pc127\\anaconda3\\lib\\site-packages (from nltk->sentence-transformers>=0.4.1->bertopic) (8.0.4)\n",
            "Requirement already satisfied: pillow>=4.1.1 in c:\\users\\pc127\\anaconda3\\lib\\site-packages (from torchvision->sentence-transformers>=0.4.1->bertopic) (9.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\pc127\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (2.4.7)\n",
            "Requirement already satisfied: colorama in c:\\users\\pc127\\anaconda3\\lib\\site-packages (from click->nltk->sentence-transformers>=0.4.1->bertopic) (0.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pc127\\anaconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\pc127\\anaconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (1.25.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pc127\\anaconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (2021.10.8)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\pc127\\anaconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (2.0.7)\n",
            "Installing collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.19.3\n",
            "    Uninstalling numpy-1.19.3:\n",
            "      Successfully uninstalled numpy-1.19.3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Ignoring invalid distribution -illow (c:\\users\\pc127\\anaconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -illow (c:\\users\\pc127\\anaconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -illow (c:\\users\\pc127\\anaconda3\\lib\\site-packages)\n",
            "    WARNING: Ignoring invalid distribution -illow (c:\\users\\pc127\\anaconda3\\lib\\site-packages)\n",
            "ERROR: Could not install packages due to an OSError: [WinError 5] Â≠òÂèñË¢´Êãí„ÄÇ: 'C:\\\\Users\\\\pc127\\\\anaconda3\\\\Lib\\\\site-packages\\\\~-mpy\\\\.libs\\\\libopenblas.QVLO2T66WEPI7JZ63PS3HMOHFEY472BC.gfortran-win_amd64.dll'\n",
            "Consider using the `--user` option or check the permissions.\n",
            "\n",
            "WARNING: Ignoring invalid distribution -illow (c:\\users\\pc127\\anaconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -illow (c:\\users\\pc127\\anaconda3\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -illow (c:\\users\\pc127\\anaconda3\\lib\\site-packages)\n"
          ]
        }
      ],
      "source": [
        "## install bertopic\n",
        "!pip install bertopic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3VGFZ1USMTu"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df =pd.read_csv(\"abstarct_data.csv\")\n",
        "df['abstract'] = df['abstract'].apply(eval)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['ËøëÂπ¥‰æÜ', 'ÈõªÂ≠êÂïÜÂ∫ó', 'Â∑≤Êàê', '‰∏ÄÁ®Æ', 'Êñ∞Ëàà', 'Èõ∂ÂîÆ', 'ÈÄöË∑Ø', 'ÈÄôÁ®Æ', '‰º¥Èö®', 'internet', 'ÁôºÂ±ï', 'Âø´ÈÄü', 'ÊàêÈï∑', 'ÈõªÂ≠êÂïÜÂãô', 'ÈÄêÊº∏', 'ÊîπËÆä', '‰∫∫È°û', 'ÁîüÊ¥ªÁøíÊÖ£', 'ÂâµÈÄ†', 'ÂïÜÊ•≠', 'Ë°åÁÇ∫', '‰∏¶Êàê', 'ËøëÂπ¥‰æÜ', '‰ºÅÊ•≠', 'Á∂ìÁáü', 'ÁÜ±ÈñÄË©±È°å', 'ÂúãÂÖßÂ§ñ', 'ÈÅéÂéª', 'Â∞çÊñº', 'ÈõªÂ≠êÂïÜÂ∫ó', 'Á†îÁ©∂', 'Â§ßÈÉ®ÂàÜ', 'ÈõÜ‰∏≠', 'Ê∂âÂÖ•', 'ÁêÜË´ñ', 'ÁßëÊäÄ', 'Êé•Âèó', 'Ê®°Âºè', 'ÊúçÂãô', 'ÂìÅË≥™', 'ËßÄÈªû', 'Êé¢Ë®é', 'ÈõªÂ≠êÂïÜÂ∫ó', 'Ê∂àË≤ªËÄÖ', 'Ë≥ºË≤∑', 'ÊÑèÈ°ò', 'ÊàñÊòØ', 'Êé¢Ë®é', 'Áü•Ë¶∫', 'È¢®Èö™', '‰ø°‰ªª', 'Ê∂àË≤ªËÄÖ', 'Ë≥ºË≤∑', 'ÊÑèÈ°ò', 'ÂΩ±Èüø', 'Èóú‰øÇ', 'ÊúâÈóú', '‰∫§ÊòìÊàêÊú¨', 'ËßÄÈªû', 'Êé¢Ë®é', 'ÈõªÂ≠êÂïÜÂ∫ó', 'Á∑ö‰∏ä', 'Ë≥ºË≤∑', 'ÊÑèÈ°ò', 'Ë≠∞È°å', 'ÁµïÂ§ßÈÉ®ÂàÜ', 'Êé¢Ë®é', '‰∫§ÊòìÊàêÊú¨', 'Áü•Ë¶∫', 'È¢®Èö™', 'Ê∂àË≤ªËÄÖ', 'Á∑ö‰∏ä', 'Ë≥ºË≤∑', 'ÊÑèÈ°ò', 'Èóú‰øÇ', 'ÊàñÊòØ', 'Êé¢Ë®é', '‰∫§ÊòìÊàêÊú¨', '‰ø°‰ªª', 'Ê∂àË≤ªËÄÖ', 'Á∑ö‰∏ä', 'Ë≥ºË≤∑', 'ÊÑèÈ°ò', 'Èóú‰øÇ', 'ÈÆÆÂ∞ë', 'Á†îÁ©∂', '‰∫§ÊòìÊàêÊú¨', 'Áü•Ë¶∫', 'È¢®Èö™', '‰ø°‰ªª', 'Ê∂àË≤ªËÄÖ', 'Á∂≤Ë∑Ø', 'Ë≥ºË≤∑', 'ÊÑèÈ°ò', 'ÂõõËÄÖ', 'Áõ∏‰∫íÂΩ±Èüø', 'Èóú‰øÇ', 'Ê∑±ÂÖ•Êé¢Ë®é', 'Âõ†Ê≠§', 'Á†îÁ©∂', 'ÁõÆÁöÑ', 'Êé¢Ë®é', '‰∫§ÊòìÊàêÊú¨', 'Ê∂àË≤ªËÄÖ', 'Á∂≤Ë∑Ø', 'Ë≥ºË≤∑', 'ÊÑèÈ°ò', 'ÂΩ±Èüø', 'Êé¢Ë®é', '‰ø°‰ªª', 'Áü•Ë¶∫', 'È¢®Èö™', 'Ê∂àË≤ªËÄÖ', 'Á∂≤Ë∑Ø', 'Ë≥ºË≤∑', 'ÊÑèÈ°ò', 'Áõ∏‰∫íÂΩ±Èüø', 'Èóú‰øÇ', 'Á†îÁ©∂', 'Á†îÁ©∂', 'ÁØÑÂúç', 'ÈáùÂ∞ç', '‰∏äÂÖ∑', 'Ë¶èÊ®°', 'ÈõªÂ≠êÂïÜÂ∫ó', 'ÁÇ∫‰∏ª', 'ÈáùÂ∞ç', 'ÊúâÁ∂≤', '‰ΩøÁî®', 'Á∂ìÈ©ó', 'ÊõæÁ∂ì', 'ÁÄèË¶Ω', 'ÈõªÂ≠êÂïÜÂ∫ó', '‰∏¶‰∏î', 'ÈõªÂ≠êÂïÜÂ∫ó', 'ÂÆåÊàê', 'ÈÅéÁöÑ', '‰∏äÁè≠Êóè', 'Â≠∏Áîü', '‰∏ªË¶Å', 'Á†îÁ©∂', 'Â∞çË±°', '‰∏¶Êé°', '‰æøÂà©', 'ÊäΩÊ®£', 'ÈÅãÁî®', '‰∏ÄËà¨', 'ÂïèÂç∑', 'Á∂≤Ë∑Ø', 'ÂïèÂç∑', 'ÈÄ≤Ë°å', 'Ë™øÊü•', 'ÂõûÊî∂', 'ÂïèÂç∑', 'ÂâîÈô§', 'ÁÑ°Êïà', 'ÂïèÂç∑', 'ÊúâÊïà', 'Âç∑Êï∏', 'ÊúâÊïà', 'ÂõûÊî∂Áéá', 'ÂàÜÂà•', 'lisrel', 'ÂàÜÊûê', 'ÈÄ≤Ë°å', 'ÂÅáË™™', 'È©óË≠â', 'Á†îÁ©∂', 'ÁôºÁèæ', 'Ê∂àË≤ªËÄÖ', 'Áü•Ë¶∫', 'È¢®Èö™', '‰∫§ÊòìÊàêÊú¨', 'Ê≠£Âêë', 'ÂΩ±Èüø', '‰ΩúÁî®', '‰ø°‰ªª', 'Áü•Ë¶∫', 'È¢®Èö™', 'ÂÖ∑Ë≤†', 'ÂΩ±Èüø', '‰ΩúÁî®', '‰ø°‰ªª', 'Ê∂àË≤ªËÄÖ', 'Á∂≤Ë∑Ø', 'Ë≥ºË≤∑', 'ÊÑèÈ°ò', 'Ê≠£Âêë', 'ÂΩ±Èüø', '‰ΩúÁî®', '‰∫§ÊòìÊàêÊú¨', 'Ê∂àË≤ªËÄÖ', 'Á∂≤Ë∑Ø', 'Ë≥ºË≤∑', 'ÊÑèÈ°ò', 'ÂÖ∑Ë≤†', 'ÂΩ±Èüø', '‰ΩúÁî®', 'Ê†πÊìö', 'Á†îÁ©∂ÊàêÊûú', 'ÊúüÂÜÄ', 'Â≠∏Ë°ì', 'ÂØ¶Âãô', 'ÊúâÊâÄ', 'Ë≤¢Áçª', 'ËóâÁî±', 'Êé¢Ë®é', '‰ø°‰ªª', 'Áü•Ë¶∫', 'È¢®Èö™', '‰∫§ÊòìÊàêÊú¨', 'Ê∂àË≤ªËÄÖ', 'Á∂≤Ë∑Ø', 'Ë≥ºË≤∑', 'ÊÑèÈ°ò', 'ÂΩ±Èüø', 'Èóú‰øÇ', 'ÈÄ≤ËÄå', 'Êèê‰æõ', 'Ê•≠ËÄÖ', 'Â¶Ç‰Ωï', 'Ê∂àË≤ªËÄÖ', 'ËßíÂ∫¶', 'Âà∂ÂÆö', '‰∏çÂêå', 'Ë°åÈä∑', 'Á≠ñÁï•', 'ÊèêÂçá', 'Ê∂àË≤ªËÄÖ', 'ÈõªÂ≠êÂïÜÂ∫ó', '‰ø°‰ªª', 'Èôç‰Ωé', 'Á∂≤Ë∑Ø', 'Ë≥ºÁâ©', '‰∫§ÊòìÊàêÊú¨', 'Áü•Ë¶∫', 'È¢®Èö™']\n"
          ]
        }
      ],
      "source": [
        "data = df['abstract']\n",
        "print(data[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "docs = []\n",
        "for d in data:\n",
        "    docs.append(\" \".join(d))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "39009"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBcNmZJzSTY8"
      },
      "source": [
        "# **Topic Modeling**\n",
        "\n",
        "In this example, we will go through the main components of BERTopic and the steps necessary to create a strong topic model. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QI6vwelqnTL-"
      },
      "source": [
        "## Training\n",
        "\n",
        "We start by instantiating BERTopic. We set language to `english` since our documents are in the English language. If you would like to use a multi-lingual model, please use `language=\"multilingual\"` instead. \n",
        "\n",
        "We will also calculate the topic probabilities. However, this can slow down BERTopic significantly at large amounts of data (>100_000 documents). It is advised to turn this off if you want to speed up the model. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "TfhfzqkoSJ1I"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Batches:   1%|          | 13/1220 [00:17<27:24,  1.36s/it]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17840/2360890527.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtopic_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBERTopic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"chinese (traditional)\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcalculate_probabilities\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mtopics\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtopic_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;32mc:\\Users\\pc127\\anaconda3\\lib\\site-packages\\bertopic\\_bertopic.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, documents, embeddings, y)\u001b[0m\n\u001b[0;32m    284\u001b[0m             self.embedding_model = select_backend(self.embedding_model,\n\u001b[0;32m    285\u001b[0m                                                   language=self.language)\n\u001b[1;32m--> 286\u001b[1;33m             embeddings = self._extract_embeddings(documents.Document,\n\u001b[0m\u001b[0;32m    287\u001b[0m                                                   \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"document\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m                                                   verbose=self.verbose)\n",
            "\u001b[1;32mc:\\Users\\pc127\\anaconda3\\lib\\site-packages\\bertopic\\_bertopic.py\u001b[0m in \u001b[0;36m_extract_embeddings\u001b[1;34m(self, documents, method, verbose)\u001b[0m\n\u001b[0;32m   1341\u001b[0m             \u001b[0membeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membed_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1342\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"document\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1343\u001b[1;33m             \u001b[0membeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membed_documents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1344\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1345\u001b[0m             raise ValueError(\"Wrong method for extracting document/word embeddings. \"\n",
            "\u001b[1;32mc:\\Users\\pc127\\anaconda3\\lib\\site-packages\\bertopic\\backend\\_base.py\u001b[0m in \u001b[0;36membed_documents\u001b[1;34m(self, document, verbose)\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[0mthat\u001b[0m \u001b[0meach\u001b[0m \u001b[0mhave\u001b[0m \u001b[0man\u001b[0m \u001b[0membeddings\u001b[0m \u001b[0msize\u001b[0m \u001b[0mof\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mm\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m         \"\"\"\n\u001b[1;32m---> 69\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;32mc:\\Users\\pc127\\anaconda3\\lib\\site-packages\\bertopic\\backend\\_sentencetransformers.py\u001b[0m in \u001b[0;36membed\u001b[1;34m(self, documents, verbose)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mthat\u001b[0m \u001b[0meach\u001b[0m \u001b[0mhave\u001b[0m \u001b[0man\u001b[0m \u001b[0membeddings\u001b[0m \u001b[0msize\u001b[0m \u001b[0mof\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mm\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \"\"\"\n\u001b[1;32m---> 63\u001b[1;33m         \u001b[0membeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_progress_bar\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0membeddings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\pc127\\anaconda3\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py\u001b[0m in \u001b[0;36mencode\u001b[1;34m(self, sentences, batch_size, show_progress_bar, output_value, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 164\u001b[1;33m                 \u001b[0mout_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    165\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0moutput_value\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'token_embeddings'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\pc127\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\pc127\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\pc127\\anaconda3\\lib\\site-packages\\sentence_transformers\\models\\Transformer.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, features)\u001b[0m\n\u001b[0;32m     64\u001b[0m             \u001b[0mtrans_features\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'token_type_ids'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'token_type_ids'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m         \u001b[0moutput_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mtrans_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m         \u001b[0moutput_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput_states\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\pc127\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\pc127\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    994\u001b[0m             \u001b[0mpast_key_values_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    995\u001b[0m         )\n\u001b[1;32m--> 996\u001b[1;33m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[0;32m    997\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    998\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\pc127\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\pc127\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    583\u001b[0m                 )\n\u001b[0;32m    584\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 585\u001b[1;33m                 layer_outputs = layer_module(\n\u001b[0m\u001b[0;32m    586\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    587\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\pc127\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\pc127\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    470\u001b[0m         \u001b[1;31m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    471\u001b[0m         \u001b[0mself_attn_past_key_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 472\u001b[1;33m         self_attention_outputs = self.attention(\n\u001b[0m\u001b[0;32m    473\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    474\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\pc127\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\pc127\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    400\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    401\u001b[0m     ) -> Tuple[torch.Tensor]:\n\u001b[1;32m--> 402\u001b[1;33m         self_outputs = self.self(\n\u001b[0m\u001b[0;32m    403\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\pc127\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\pc127\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    328\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    329\u001b[0m         \u001b[1;31m# Normalize the attention scores to probabilities.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 330\u001b[1;33m         \u001b[0mattention_probs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattention_scores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    331\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    332\u001b[0m         \u001b[1;31m# This is actually dropping out entire tokens to attend to, which might\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\pc127\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36msoftmax\u001b[1;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[0;32m   1510\u001b[0m         \u001b[0mdim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_softmax_dim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'softmax'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1511\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1512\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1513\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1514\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from bertopic import BERTopic\n",
        "\n",
        "topic_model = BERTopic(language=\"chinese (traditional)\", calculate_probabilities=True, verbose=True)\n",
        "topics, probs = topic_model.fit_transform(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'Python 3.10.4 ('heroku')' requires ipykernel package.\n",
            "Run the following command to install 'ipykernel' into the Python environment. \n",
            "Command: 'conda install -n heroku ipykernel --update-deps --force-reinstall'"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "np.argmax(probs[0])\n",
        "# topics[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'Python 3.10.4 ('heroku')' requires ipykernel package.\n",
            "Run the following command to install 'ipykernel' into the Python environment. \n",
            "Command: 'conda install -n heroku ipykernel --update-deps --force-reinstall'"
          ]
        }
      ],
      "source": [
        "import pickle \n",
        "pickle.dump((topics,probs),open(\"bert_topic1.pk\",\"wb\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJs94tXDo4f6"
      },
      "source": [
        "**NOTE**: Use `language=\"multilingual\"` to select a model that support 50+ languages."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5O3KpHTnVpz"
      },
      "source": [
        "## Extracting Topics\n",
        "After fitting our model, we can start by looking at the results. Typically, we look at the most frequent topics first as they best represent the collection of documents. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle \n",
        "topics,probs = pickle.load(open(\"bert_topic1.pk\",\"rb\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "ScBUgXn06IK6",
        "outputId": "ee49a346-8d12-41c4-c7e9-8c1b6782c636"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'Python 3.10.4 ('heroku')' requires ipykernel package.\n",
            "Run the following command to install 'ipykernel' into the Python environment. \n",
            "Command: 'conda install -n heroku ipykernel --update-deps --force-reinstall'"
          ]
        }
      ],
      "source": [
        "freq = topic_model.get_topic_info(); freq.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'Python 3.10.4 ('heroku')' requires ipykernel package.\n",
            "Run the following command to install 'ipykernel' into the Python environment. \n",
            "Command: 'conda install -n heroku ipykernel --update-deps --force-reinstall'"
          ]
        }
      ],
      "source": [
        "topic_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BtOgifV7Q-H"
      },
      "source": [
        "-1 refers to all outliers and should typically be ignored. Next, let's take a look at a frequent topic that were generated:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IVpvT4bA6KiN",
        "outputId": "9cf99b89-30bb-45fe-b98b-063f8f3624d9"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'Python 3.10.4 ('heroku')' requires ipykernel package.\n",
            "Run the following command to install 'ipykernel' into the Python environment. \n",
            "Command: 'conda install -n heroku ipykernel --update-deps --force-reinstall'"
          ]
        }
      ],
      "source": [
        "topic_model.get_topic(5)  # Select the most frequent topic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixc-X2JzodrZ"
      },
      "source": [
        "**NOTE**: BERTopic is stocastich which mmeans that the topics might differ across runs. This is mostly due to the stocastisch nature of UMAP."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbT2Bd9gqaJ3"
      },
      "source": [
        "# **Visualization**\n",
        "There are several visualization options available in BERTopic, namely the visualization of topics, probabilities and topics over time. Topic modeling is, to a certain extent, quite subjective. Visualizations help understand the topics that were created. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8c8LenB8Zyl"
      },
      "source": [
        "## Visualize Topics\n",
        "After having trained our `BERTopic` model, we can iteratively go through perhaps a hundred topic to get a good \n",
        "understanding of the topics that were extract. However, that takes quite some time and lacks a global representation. \n",
        "Instead, we can visualize the topics that were generated in a way very similar to \n",
        "[LDAvis](https://github.com/cpsievert/LDAvis):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 667
        },
        "id": "S9qDqEHddgKq",
        "outputId": "3fddd5f1-194e-4708-a7dc-f0c5602c140a"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'Python 3.10.4 ('heroku')' requires ipykernel package.\n",
            "Run the following command to install 'ipykernel' into the Python environment. \n",
            "Command: 'conda install -n heroku ipykernel --update-deps --force-reinstall'"
          ]
        }
      ],
      "source": [
        "topic_model.visualize_topics()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'Python 3.10.4 ('heroku')' requires ipykernel package.\n",
            "Run the following command to install 'ipykernel' into the Python environment. \n",
            "Command: 'conda install -n heroku ipykernel --update-deps --force-reinstall'"
          ]
        }
      ],
      "source": [
        "len(topic_model.topics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITB7bf6q8nWQ"
      },
      "source": [
        "## Visualize Topic Probabilities\n",
        "\n",
        "The variable `probabilities` that is returned from `transform()` or `fit_transform()` can \n",
        "be used to understand how confident BERTopic is that certain topics can be found in a document. \n",
        "\n",
        "To visualize the distributions, we simply call:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "ypfI1-KHdmcX",
        "outputId": "4fd56127-4143-4cf7-d227-ef43725f1ad7"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'Python 3.10.4 ('heroku')' requires ipykernel package.\n",
            "Run the following command to install 'ipykernel' into the Python environment. \n",
            "Command: 'conda install -n heroku ipykernel --update-deps --force-reinstall'"
          ]
        }
      ],
      "source": [
        "topic_model.visualize_distribution(probs[200], min_probability=0.015)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHRTeSpl5JYB"
      },
      "source": [
        "## Visualize Topic Hierarchy\n",
        "\n",
        "The topics that were created can be hierarchically reduced. In order to understand the potential hierarchical structure of the topics, we can use scipy.cluster.hierarchy to create clusters and visualize how they relate to one another. This might help selecting an appropriate nr_topics when reducing the number of topics that you have created."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        },
        "id": "ltmLFRR56a4X",
        "outputId": "0458e59c-30ce-4700-8dbf-0932afc63f1d"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'Python 3.10.4 ('heroku')' requires ipykernel package.\n",
            "Run the following command to install 'ipykernel' into the Python environment. \n",
            "Command: 'conda install -n heroku ipykernel --update-deps --force-reinstall'"
          ]
        }
      ],
      "source": [
        "topic_model.visualize_hierarchy(top_n_topics=50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4spXl2_C6flq"
      },
      "source": [
        "## Visualize Terms\n",
        "\n",
        "We can visualize the selected terms for a few topics by creating bar charts out of the c-TF-IDF scores for each topic representation. Insights can be gained from the relative c-TF-IDF scores between and within topics. Moreover, you can easily compare topic representations to each other."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        },
        "id": "zpm9LsKW6mi5",
        "outputId": "1197affc-dde2-44c1-9ba7-c9fb36a1143c"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'Python 3.10.4 ('heroku')' requires ipykernel package.\n",
            "Run the following command to install 'ipykernel' into the Python environment. \n",
            "Command: 'conda install -n heroku ipykernel --update-deps --force-reinstall'"
          ]
        }
      ],
      "source": [
        "topic_model.visualize_barchart(top_n_topics=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCPdi6_z6sbT"
      },
      "source": [
        "## Visualize Topic Similarity\n",
        "Having generated topic embeddings, through both c-TF-IDF and embeddings, we can create a similarity matrix by simply applying cosine similarities through those topic embeddings. The result will be a matrix indicating how similar certain topics are to each other."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "edzNhZuZ6wTr",
        "outputId": "e01231db-fe82-49d3-a96f-8135522d9b9b"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'Python 3.10.4 ('heroku')' requires ipykernel package.\n",
            "Run the following command to install 'ipykernel' into the Python environment. \n",
            "Command: 'conda install -n heroku ipykernel --update-deps --force-reinstall'"
          ]
        }
      ],
      "source": [
        "topic_model.visualize_heatmap(n_clusters=20, width=1000, height=1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ak_CLR164mx"
      },
      "source": [
        "## Visualize Term Score Decline\n",
        "Topics are represented by a number of words starting with the best representative word. Each word is represented by a c-TF-IDF score. The higher the score, the more representative a word to the topic is. Since the topic words are sorted by their c-TF-IDF score, the scores slowly decline with each word that is added. At some point adding words to the topic representation only marginally increases the total c-TF-IDF score and would not be beneficial for its representation.\n",
        "\n",
        "To visualize this effect, we can plot the c-TF-IDF scores for each topic by the term rank of each word. In other words, the position of the words (term rank), where the words with the highest c-TF-IDF score will have a rank of 1, will be put on the x-axis. Whereas the y-axis will be populated by the c-TF-IDF scores. The result is a visualization that shows you the decline of c-TF-IDF score when adding words to the topic representation. It allows you, using the elbow method, the select the best number of words in a topic.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 517
        },
        "id": "7gT3Korh6-MX",
        "outputId": "5810f810-4633-425e-a7d2-3a1e9570639a"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'Python 3.10.4 ('heroku')' requires ipykernel package.\n",
            "Run the following command to install 'ipykernel' into the Python environment. \n",
            "Command: 'conda install -n heroku ipykernel --update-deps --force-reinstall'"
          ]
        }
      ],
      "source": [
        "topic_model.visualize_term_rank()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D48ienfZrfP0"
      },
      "source": [
        "# **Topic Representation**\n",
        "After having created the topic model, you might not be satisfied with some of the parameters you have chosen. Fortunately, BERTopic allows you to update the topics after they have been created. \n",
        "\n",
        "This allows for fine-tuning the model to your specifications and wishes. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4m3UMsw-Zxk"
      },
      "source": [
        "## Update Topics\n",
        "When you have trained a model and viewed the topics and the words that represent them,\n",
        "you might not be satisfied with the representation. Perhaps you forgot to remove\n",
        "stopwords or you want to try out a different `n_gram_range`. We can use the function `update_topics` to update \n",
        "the topic representation with new parameters for `c-TF-IDF`: \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KWm7B-FJ-iYW"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'Python 3.10.4 ('heroku')' requires ipykernel package.\n",
            "Run the following command to install 'ipykernel' into the Python environment. \n",
            "Command: 'conda install -n heroku ipykernel --update-deps --force-reinstall'"
          ]
        }
      ],
      "source": [
        "topic_model.update_topics(data, topics, n_gram_range=(1, 2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wf31gQavdtfG",
        "outputId": "b2cf3d47-f665-44fe-bbe0-c927e4d73363"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'Python 3.10.4 ('heroku')' requires ipykernel package.\n",
            "Run the following command to install 'ipykernel' into the Python environment. \n",
            "Command: 'conda install -n heroku ipykernel --update-deps --force-reinstall'"
          ]
        }
      ],
      "source": [
        "topic_model.get_topic(0)   # We select topic that we viewed before"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9antKpdC91A-"
      },
      "source": [
        "## Topic Reduction\n",
        "We can also reduce the number of topics after having trained a BERTopic model. The advantage of doing so, \n",
        "is that you can decide the number of topics after knowing how many are actually created. It is difficult to \n",
        "predict before training your model how many topics that are in your documents and how many will be extracted. \n",
        "Instead, we can decide afterwards how many topics seems realistic:\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5m4Nd7Us-Peg",
        "outputId": "2d442d39-5bc5-4e7f-c155-6e6c664f6714"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'Python 3.10.4 ('heroku')' requires ipykernel package.\n",
            "Run the following command to install 'ipykernel' into the Python environment. \n",
            "Command: 'conda install -n heroku ipykernel --update-deps --force-reinstall'"
          ]
        }
      ],
      "source": [
        "new_topics, new_probs = topic_model.reduce_topics(data, topics, probs, nr_topics=60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXYJ745O-03Z"
      },
      "source": [
        "# **Search Topics**\n",
        "After having trained our model, we can use `find_topics` to search for topics that are similar \n",
        "to an input search_term. Here, we are going to be searching for topics that closely relate the \n",
        "search term \"vehicle\". Then, we extract the most similar topic and check the results: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lAdiVYej-2i-",
        "outputId": "c107bd7d-1025-4d4d-b179-d007c7d16cd2"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'Python 3.10.4 ('heroku')' requires ipykernel package.\n",
            "Run the following command to install 'ipykernel' into the Python environment. \n",
            "Command: 'conda install -n heroku ipykernel --update-deps --force-reinstall'"
          ]
        }
      ],
      "source": [
        "similar_topics, similarity = topic_model.find_topics(\"‰∫∫Â∑•Êô∫ÊÖß\", top_n=10); similar_topics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9paNa09d3Xy",
        "outputId": "545bbd07-8850-46d4-fdb7-28cb3d158b15"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'Python 3.10.4 ('heroku')' requires ipykernel package.\n",
            "Run the following command to install 'ipykernel' into the Python environment. \n",
            "Command: 'conda install -n heroku ipykernel --update-deps --force-reinstall'"
          ]
        }
      ],
      "source": [
        "for topic in similar_topics:\n",
        "    print(topic_model.get_topic(topic))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'Python 3.10.4 ('heroku')' requires ipykernel package.\n",
            "Run the following command to install 'ipykernel' into the Python environment. \n",
            "Command: 'conda install -n heroku ipykernel --update-deps --force-reinstall'"
          ]
        }
      ],
      "source": [
        "new_doc = \"By semantically optimizing your content, you add more meaning to the words you use. You optimize for the true intent of your users, not just answering a simple query. This means answering the first question, then answering the second, third, fourth, and fifth questions right after that.\"\n",
        "\n",
        "topic_model.transform([new_doc])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wekNoQNuUVoU"
      },
      "source": [
        "# **Model serialization**\n",
        "The model and its internal settings can easily be saved. Note that the documents and embeddings will not be saved. However, UMAP and HDBSCAN will be saved. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nWUF1uxiSb_a"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'Python 3.10.4 ('heroku')' requires ipykernel package.\n",
            "Run the following command to install 'ipykernel' into the Python environment. \n",
            "Command: 'conda install -n heroku ipykernel --update-deps --force-reinstall'"
          ]
        }
      ],
      "source": [
        "# Save model\n",
        "topic_model.save(\"my_model\")\t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y_eHBI1jSb6i"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'Python 3.10.4 ('heroku')' requires ipykernel package.\n",
            "Run the following command to install 'ipykernel' into the Python environment. \n",
            "Command: 'conda install -n heroku ipykernel --update-deps --force-reinstall'"
          ]
        }
      ],
      "source": [
        "# Load model\n",
        "my_model = BERTopic.load(\"my_model\")\t"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2eyImbal7lb8"
      },
      "source": [
        "# **Embedding Models**\n",
        "The parameter `embedding_model` takes in a string pointing to a sentence-transformers model, a SentenceTransformer, or a Flair DocumentEmbedding model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZKyW7NZpnEk"
      },
      "source": [
        "## Sentence-Transformers\n",
        "You can select any model from sentence-transformers here and pass it through BERTopic with embedding_model:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k7sPgNfzprbP"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'Python 3.10.4 ('heroku')' requires ipykernel package.\n",
            "Run the following command to install 'ipykernel' into the Python environment. \n",
            "Command: 'conda install -n heroku ipykernel --update-deps --force-reinstall'"
          ]
        }
      ],
      "source": [
        "topic_model = BERTopic(embedding_model=\"xlm-r-bert-base-nli-stsb-mean-tokens\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vv7i1JTp62V"
      },
      "source": [
        "Or select a SentenceTransformer model with your own parameters:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eh5qp58Hp7Ua"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'Python 3.10.4 ('heroku')' requires ipykernel package.\n",
            "Run the following command to install 'ipykernel' into the Python environment. \n",
            "Command: 'conda install -n heroku ipykernel --update-deps --force-reinstall'"
          ]
        }
      ],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "sentence_model = SentenceTransformer(\"distilbert-base-nli-mean-tokens\", device=\"cpu\")\n",
        "topic_model = BERTopic(embedding_model=sentence_model, verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SoMc1W-x7-b5"
      },
      "source": [
        "Click [here](https://www.sbert.net/docs/pretrained_models.html) for a list of supported sentence transformers models.  \n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "BERTopic.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.8.5 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "20cb42a84e7b6e574f03065abd5a3d955575a609c691c32a5a1391bb00c20c6e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
